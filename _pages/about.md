---
permalink: /
title: "Khai-Nguyen Nguyen"
excerpt: "Khai-Nguyen Nguyen"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
I am a CS and Engineering student at Bucknell University at Lewisburg, Pennsylvania. I am particularly interested in methods of improving existing models' learning to reach human-level language understanding. To this end, I hope to do research on the **interpretability and explainability** and **representation learning** of NLP models.

I am fortunate to be advised by Professor Alex Kelly (Carleton University) and Professor Thiago Serra (Bucknell University) in my previous research.


My work
===
<style>
body {
    font-family: "Helvetica Neue", Arial, sans-serif;
    font-size: 14px;
    line-height: 1.6;
    color: #333;
}

code {
    font-family: "Source Code Pro", monospace;
    font-size: 12px;
    background-color: #f9f9f9;
    padding: 2px 4px;
    border-radius: 4px;
    color: #c7254e;
    background-color: #f9f2f4;
}

a {
    color: #3498db;
    text-decoration: none;
}

a:hover {
    text-decoration: underline;
}
</style>

# Human-Level Language Understanding in Models

What does it mean for a model to achieve human-level language understanding, and how do we get there? While the first part of the question is more philosophical, I believe one direction toward the answer to the second part lies in **adopting elements from human language learning** into models.

Toward this end, my research projects in NLP have explored [using visually grounded information to learn better representation in multilingual context](https://arxiv.org/abs/2210.05487) — a technique motivated by the multimodal learning in humans — and [summarizing past texts and incorporating them into the current input](https://github.com/toontran/limitless-sequence-modeling) to capture long-range dependencies — a phenomenon utilized by writers and storytellers in their works.

Towards the answer to the first question, I believe interpretability is of great importance. Many models are said to perform certain tasks at the same level as humans, if not better. However, what attributes to their success? What is the rationale behind their outputs? If we can answer these questions, we are one step closer to understanding the capabilities of the state-of-the-arts, and subsequently could work toward improving them.

My related experience on this topic mainly deals with feedforward networks, but I am exploring ways to extend the methods developed with Professor Serra to the transformer architecture.


CS education
===
I am also interested in work on activities related to education in CS. I have been a teaching assistant for CS2: Data Structures and Algorithms for Bucknell's CS Department for two semesters, which I really enjoyed. I also did research on the identifying which topics are important in CS2 and which are difficult. I hope to be a TA in my graduate studies.


Contact
===
You can view my CV [here](https://drive.google.com/file/d/1_JP0uLViGp4pLtpqtSFBK8w2Ozj1iiRX/view?usp=sharing). Please feel free to contact me via email (nkn002@bucknell.edu)!

Updates
===

**March, 2023** <br>
Our Getting away paper was accepted to ICLR SNN 2023!

**Feb, 2023** <br>
Our Getting away paper was accepted to CPAIOR 2023! <br>
Our Like a bilingual baby paper is now under submission to CogSci 2023



Papers
====
**Getting away with more network pruning: From sparsity to geometry and linear regions** <br>
Jeffrey Cai, **Khai-Nguyen Nguyen**, Nishant Shrestha, Aidan Good, Ruisen Tu, Xin Yu, Shandian Zhe, Thiago Serra <br>
_Accepted to CPAIOR 2023_ <br>
**Note:** co-first authored

**Like a bilingual baby: The advantage of visually grounding a bilingual language model** <br>
**Khai-Nguyen Nguyen**, Zixin Tang, Ankur Mali, M Alex Kelly<br>
_Submitted to CogSci 2023_

**Important and Difficult Topics in CS2: An Expert Consensus via Delphi Study** <br>
Lea Wittie, Anastasia Kurdia, Meriel Huggard, **Khai-Nguyen Nguyen** <br>
_Submitted to the ASEE Annual Conference and Exposition 2023_


