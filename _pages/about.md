---
permalink: /
title: "About me"
excerpt: "Khai-Nguyen Nguyen"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a CS and Engineering student at Bucknell University at Lewisburg, Pennsylvania. I am particularly interested in methods of improving existing models' learning to reach human-level language understanding. To this end, I hope to do research on the **interpretability and explainability** and **representation learning** of NLP models.

My work
===
What does it mean for a model to achieve human-level language understanding, and how do we get there? While the first part of the question is more philosophical, I believe one direction toward the answer to the second part lies in **adopting elements from human language learning** into models. Toward this end, my research projects in NLP have explored ([1](https://arxiv.org/abs/2210.05487)) using visually grounded information to learn better representation in multilingual context - a technique motivated by the multimodal learning in humans - and ([2](https://github.com/toontran/limitless-sequence-modeling)) summarizing past texts and incorporate them to the current input to capture long-range dependencies - a phenomenom utilized by writers and story-tellers in their works. 

Towards the answer to the first question, I believe that the study interpretability and explainability is of great importance. Many models are said to perform certain tasks at the same level as humans, if not better. However, what attributed to their success? What is the rationale behind their outputs? How do they understand text and if there are noises, do they still perform as well as before? If we are able to answer these questions, we are one step closer to understand the capabilities of the state-of-the-arts, and subsequently could work toward improving them. My related experience on this topic mainly deals with feedforward networks, but I am exploring ways to extending the methods I developed with Professor Serra's lab to the transformer architecture.

Though many have criticized Deep Learning and specifically NLP as black boxes, I am motivated by Professor David Bau's thoughts on his [AMA](https://www.reddit.com/r/IAmA/comments/r5vte5/i_am_david_bau_and_i_study_the_structure_of_the/) on Reddit. Particularly, I believe that _there may be a ripe research opportunity inside that [messy, unwieldy black box]_.


CS education
===
I am also interested in work on activities related to education in CS. I have been a teaching assistant for CS2: Data Structures and Algorithms for Bucknell's CS Department for two semesters, which I really enjoyed. I also did research on the identifying which topics are important in CS2 and which are difficult. I hope to be a TA in my graduate studies.


Contact
===
You can view my CV [here](https://drive.google.com/file/d/1_JP0uLViGp4pLtpqtSFBK8w2Ozj1iiRX/view?usp=sharing). Please feel free to contact me via email (nkn002@bucknell.edu)!

<!-- About me
===
I am originally from Ho Chi Minh City, Vietnam. Here in the States, I go by Nguyen Nguyen, but in Vietnamese, my name is Khải Nguyên [kʰaːj ŋwiəŋ]. As Vietnamese is a tonal and isolating language, many people in the States have found it difficult to pronounce my name. As such, I have enjoyed hearing many variations of my name, such as win, wind, or en-win. Nevertheless, my favorites have been win-win, which is the main motivation for my English name, and Winnie-the-Pooh. Since language changes with time, I believe there is no absolute "correct" way to pronounce any word, so please feel free to call me whatever you want - as long as it makes sense. -->

Papers
====

**Getting away with more network pruning: From sparsity to geometry and linear regions** <br>
Jeffrey Cai*, **Khai-Nguyen Nguyen***, Nishant Shrestha, Aidan Good, Ruisen Tu, Xin Yu, Shandian Zhe, Thiago Serra <br>
_Submitted to the International Conference on the Integration of Constraint Programming, Artificial Intelligence,
and Operations Research (CPAIOR) 2023_ <br>
**Note:** _* denotes equal contribution_

**Like a bilingual baby: The advantage of visually grounding a bilingual language model** <br>
**Khai-Nguyen Nguyen**, Zixin Tang, Ankur Mali, M Alex Kelly<br>
_In arxiv 2022_

**Important and Difficult Topics in CS2: An Expert Consensus via Delphi Study** <br>
Lea Wittie, Anastasia Kurdia, Meriel Huggard, **Khai-Nguyen Nguyen** <br>
_Submitted to the ASEE Annual Conference and Exposition 2023_




